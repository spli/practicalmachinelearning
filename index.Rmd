---
title: "Practical Machine Learning Course Project"
author: "Simon Frost"
date: "26 August 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
set.seed(33633)

library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) 
registerDoParallel(cluster)
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv","pml-training.csv",method ="curl")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv","pml-testing.csv",method ="curl")
```

## Summary

In this assignment I will aim to use data generated by the Human Activity Recognition project based on data generated by accelerometers on belt, forearm, arm, and dumbell of 6 participants. 

## Initial Data Exploration
First I load in both the training and testing data sets and ensure that all variables are numeric. We also need to remove columns from the training set and testing set that we don't have data for and those not related to accelerometer data; these and subject related data, which I feel is not relevant to the predictor (e.g we are not trying to predict who is good at lifting dumbells).

```{r loadData, echo =F}
trainingAll <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")

removeCols <- c(colnames(testing)[1:7], colnames(testing)[colSums(is.na(testing)) > 0])

trainingAll <- trainingAll[,!colnames(trainingAll) %in% removeCols]
testing <- testing[,!colnames(testing) %in% removeCols]

trainingAll <- trainingAll[ , colSums(is.na(trainingAll)) == 0]


trainingAll[,-dim(trainingAll)[2]] <- sapply(trainingAll[-dim(trainingAll)[2]], as.numeric)

testing[,-dim(testing)[2]] <- sapply(testing[-dim(testing)[2]], as.numeric)
```

This leaves us with the following dataset size

```{r size}
dim(trainingAll)
```

```{r correlation, echo=F}
M <- abs(cor(trainingAll[,-dim(trainingAll)[2]]))
diag(M) <- 0
corr <- nrow(which(M > 0.8, arr.ind=T))
```

A quick analysis of the variables show there is a high degree of correlation between the variables, if we do quick correlation matrix we see `corr` correlated pairs, This is because that many of the variables are actually total values derived from others. We therefore remove these from the training data.

```{r total}
trainingAll <- trainingAll[,-grep("*total*",colnames(trainingAll))]
testing <- testing[,-grep("*total*",colnames(testing))]

```

We're still left with a high level of correlation, so we'll apply a pre-processing step of PCA to weight our predictors accordingly.

We finally split the training data set into a training and validation set.

```{r split}
inTrain <- createDataPartition(trainingAll$classe, p = 0.8, list = F)
training <- trainingAll[inTrain, ]
validation <- trainingAll[-inTrain, ]
```

## Model Selection and Cross-validation

We'll build two models: Regression Trees and Random forests, then obtain the accuracy and out of sample errors by cross-validation. As mentioned above we'll pre-process with pca.

```{r models}
modRpart <- train(classe ~ ., data = training, method ="rpart", preProcess = "pca")
predRpart <- predict(modRpart, validation)

modRf <- train(classe ~. , data = training, preProcess = "pca", method = "rf", prox = T, 
               trControl = trainControl(method = "cv",
                           number = 10,
                           allowParallel = TRUE))
predRf <- predict(modRf, validation)
```

```{r accuracy, echo=F}
accuracyRates = c(sum(predRpart == validation$classe)/length(validation$classe),
                  sum(predRf == validation$classe)/length(validation$classe))
outOfSampleRates = 1 - accuracyRates
data.frame(accuracyRates, outOfSampleRates,row.names = c("rpart","rf"))
```

We can see the accuracy and out of sample error rates of the models here:

```{r rates}
data.frame(accuracyRates, outOfSampleRates,row.names = c("rpart","rf"))
```

As we can see, the Random Forests method has a much better accuracy and is a much better model for prediction.

## Results

Using our model above we make the following predictions for class on the testing set:

```{r test}
data.frame(classe = predict(modRf, newdata = testing))
```
